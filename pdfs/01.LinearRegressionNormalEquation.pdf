\documentclass[12pt]{extarticle}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{minted}

\graphicspath{{./images}}

\title{Linear Regression} 
\author{}
\date{6 February 2023}

\setlength{\parskip}{5pt}
\begin{document}

\maketitle

\section{Introduction}
In the last post, we saw different types of supervised learning algorithms i.e. Regression and Classification. Today, we're going to explore one of the most famous supervised learning algorithms i.e. \textbf{Linear Regression}.

\begin{wrapfigure}{r}{0.25\textwidth} %this figure will be at the right
    \centering
    \includegraphics[width=0.3\textwidth]{LinearRegression}
\end{wrapfigure}

\textbf{Linear Regression} is used for tackling regression problems i.e. when we want to predict a real-valued or continuous output from the given set of features. It is usually used when there is a linear relation between different sets of features and the output.

A linear relationship is a statistical term used to describe a straight-line relationship between two variables.
In other words, when we plot these features and output on the graph we can fit a straight line through these points.

\section{Hypothesis Function}
It is also known as the predictor function which takes features as input and predicts the output. 

In the case of Linear Regression, this function represents the line that fits through the given data points such that it minimizes the difference between the predicted value and the real value.
The difference between the predicted value and the real value is termed as Error.

It is denoted by h(x),

\begin{align*}
    h(x) &= \theta_0 + \theta_1x_1 + ... + \theta_nx_n \\
    h(x) &= \sum_{i=0}^{n}\theta_ix_i \text{, where } x_0 = 1
\end{align*}

Now for 'm' features, weâ€™re going to have 'm' hypothesis functions:

\begin{align*}
    h(x)_1 &= \theta_0 + \theta_1x_{11} + ... + \theta_nx_{1n} \\
    h(x)_2 &= \theta_0 + \theta_1x_{21} + ... + \theta_nx_{2n} \\
    h(x)_3 &= \theta_0 + \theta_1x_{31} + ... + \theta_nx_{3n} \\
    \vdots \\
    h(x)_m &= \theta_0 + \theta_1x_{m1} + ... + \theta_nx_{mn} \\
\end{align*}

We have a convenient way to represent these equations using matrices:

$$H = X\Theta$$
where,

$H = \begin{bmatrix}
    h(x)_1 \\
    h(x)_2 \\
    h(x)_3 \\
    \vdots\\
    h(x)_m
\end{bmatrix}$, denotes all of the predicted values

$X = \begin{bmatrix}
    1 & x_{11} & x_{12} & \ldots & x_{1n} \\
    1 & x_{21} & x_{22} & \ldots & x_{2n} \\
    1 & x_{31} & x_{32} & \ldots & x_{3n} \\
    \vdots \\
    1 & x_{m1} & x_{m2} & \ldots & x_{mn} \\
\end{bmatrix}$, denotes all of the feature values

$\Theta = \begin{bmatrix}
    \theta_0 \\
    \theta_1 \\
    \theta_2 \\
    \vdots \\
    \theta_n \\
\end{bmatrix}$, denotes the coefficients of the hypothesis function

\section{Cost Function}
The Cost Function, also known as \textbf{Loss Function}, is used to evaluate the performance of our current model. The Cost Function also aids us in minimizing the error and hence, finding the values of $\Theta$.

For regression, some of the available cost functions are Mean Error(ME), Mean Absolute Error(MAE), MSE(Mean Squared Error), etc. Out of all these cost functions, we mostly use MSE.

MSE is denoted as:
$$J(\theta_0,\theta_1,...,\theta_n) = \frac 1 {2m} \sum_{i=1}^{m}(h_\theta(x)_i-y_i)^2$$

We'll see more about this in Gradient Descent.

\section{Normal Equation}
This equation directly provides us with the optimal value of theta, so that the error is minimized.
$$\Theta = \big(X^TX\big)^{-1}X^TY$$
where,
$Y = \begin{bmatrix}
    y_1 \\
    y_2 \\
    y_3 \\
    \vdots \\
    y_n \\
\end{bmatrix}$, denotes all of the real values

If you want to know how this equation is derived, refer \href{http://mlwiki.org/index.php/Projection_onto_Subspaces}{here}.

Now as we have the optimal value of theta, we can plug in these theta values in our hypothesis function, and finally, we have our hypothesis function. Now we can just plug in new feature values in the hypothesis function and can get the predicted output.

\section{Python Implementation}

First, we'll create an example dataset using numpy.

\begin{minted}{python}
    import numpy as np
    import matplotlib.pyplot as plt
    X = np.random.rand(100, 1)            # features
    y = 3 + 2*X + np.random.rand(100, 1)  # labels
    plt.plot(X, y, 'bo')
    plt.axis([0,1,3,6])
\end{minted}

\includegraphics[scale=0.5]{images/SampleData.png}

Now, we need to add 1's in first column of X and then we're using normal equation to get the value of theta.
\begin{minted}{python}
    >>> X_biased = np.c_[np.ones(100), X]
    >>> theta = np.linalg.inv(X_biased.T.dot(X_biased)).dot(X_biased.T.dot(y))
    >>> theta
    array([[3.51894288],
       [1.91539013]])
\end{minted}

Now, we can predict output for new features.
\begin{minted}{python}
    >>> X_new = [[2], [3]]
    >>> X_new_biased = np.c_[np.ones(2), X_new]
    >>> y_new = X_new_biased.dot(theta)
    >>> y_new
    array([[7.34972315],
       [9.26511329]])
\end{minted}

Drawing our Hypothesis Function on graph.
\begin{minted}{python}
    plt.plot(X, y, 'bo')
    plt.plot(X_new, y_new, 'r-')
    plt.axis([0,1,3,6])
    plt.show()
\end{minted}

\includegraphics[scale=0.5]{images/SampleDataRegression.png}

\section{Conclusion}
So, today we learnt about Linear Regression using Normal Equation. In the upcoming posts, we'll discuss Gradient Descent and it's different types.
If you have any doubts regarding Linear Regression, feel free to reach out to me and let's discuss.

Let's keep this MLM going on and wishing you a great week ahead!

\end{document}

